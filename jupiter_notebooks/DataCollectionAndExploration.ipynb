{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Collection Notebook**\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Fetch data from [Kaggle](https://www.kaggle.com/) and save it as raw data.\n",
    "+ Inspect and save the data under outputs/datasets/collection.\n",
    "+ Gain a deeper understanding of the data using Pandas Profiling and correlation analysis to address Business Requirement 1:\n",
    "    + The client wants to identify which variables have the strongest correlation with loan defaults.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "+ Authentication token from Kaggle (JSON file).\n",
    "+ Kaggle dataset: Loan Default Dataset.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "+ Generate a dataset in the outputs file.\n",
    "+ Generate code that answer business requirement 1.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change working directory\n",
    "\n",
    "Change working directory from the current one to the parent folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd() # get current directory\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the parent directory the current directory, we must use `os.path.dirname()` to get the parent, and `os.chir()` to redefine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir)) # change directory to parent directory\n",
    "print(\"The directory you are in is:\", os.getcwd()) # print current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd() # get current directory\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the data from Kaggle\n",
    "\n",
    "First install Kaggle package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ An account must be registered on Kaggle to obtain an API Key in the format of a JSON file.\n",
    "\n",
    "+ To authenticate with the Kaggle API, set the environment variable `KAGGLE_CONFIG_DIR` to the current working directory. It is necessary to modify its permissions to read§write for the owner, using `chmod 600` to restrict access and protect sensitive credentials.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
    "! chmod 600 kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The dataset used in this project is [Loan Default Dataset](https://www.kaggle.com/datasets/yasserh/loan-default-dataset/data).\n",
    "\n",
    "+ The dataset path is `yasserh/loan-default-dataset/data`.\n",
    "\n",
    "+ Define the Kaggle dataset and destination folder and download it to the folder (inputs/datasets/raw)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KaggleDatasetPath = \"yasserh/loan-default-dataset\"\n",
    "DestinationFolder = \"inputs/datasets/raw\"   \n",
    "! kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the downloaded file, delete the zip and the kaggle.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip {DestinationFolder}/*.zip -d {DestinationFolder} \\\n",
    "  && rm {DestinationFolder}/*.zip \\\n",
    "  && rm kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Inspect the data\n",
    "\n",
    "Using pandas library, the dataset can be loaded as a dataframe so the data can be inspected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"inputs/datasets/raw/Loan_Default.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataframe summary can be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ From the summary we can see that there are missing values in the dataframe, as the column \"Non-null\" have different values for different features. \n",
    "\n",
    "+ We create and print a list with the columns that contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_nan = df.columns[df.isnull().any()].to_list()\n",
    "columns_with_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ As the dataset contains an ID column, we must check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(subset=[\"ID\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The status variable is already numerical, meaning there is no need for changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will I need to convert any columns to a different data type?\n",
    "# will I need to rename any columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  os.makedirs(name='outputs/datasets/collection') # create outputs/datasets/collection folder\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "df.to_csv(f\"outputs/datasets/collection/LoanDefault.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "We will conduct pandas profiling to explore the dataset, identify missing values, analyze data types and distributions, and understand the business context of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "pandas_report = ProfileReport(df=df, minimal=True)\n",
    "pandas_report.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After profiling, we can draw some conclusions:\n",
    "+ Almost half of the features have more than 85% of values concentrated in a single category. These may provide little predictive power and could be dropped after further evaluation.\n",
    "+ Important features such as property_value, income, loan-to-value(LTV), term and age contain missing or incorrect values that will be addressed.\n",
    "+ In the alerts tab, a warning is raised for 75% of zeros in the status column. This is expected since status is our target variable, where 0 indicates \"not defaulted.\"\n",
    "+ LTV shows extremely high skewness, suggesting that most values are concentrated at lower levels, while a few extreme outliers—likely errors—significantly distort the distribution.\n",
    "+ Other alerts, including missing values, high uniqueness, or constant features, will be handled later through imputation or removal as needed.\n",
    "\n",
    "## Correlation Study\n",
    "\n",
    "Running Pearson and Spearman correlation will help identify key predictors of default status before cleaning the data. Pearson detects linear relationships, while Spearman captures rank-based trends. This will guide feature selection and highlight potential redundancies or outliers.\n",
    "\n",
    "Before conducting the correlation studies, the data needs to be converted to numerical format. Missing values will be imputed with the most frequent value in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import CategoricalImputer\n",
    "\n",
    "imputer = CategoricalImputer(imputation_method=\"frequent\")\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "df_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.encoding import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(variables=df_imputed.columns[df_imputed.dtypes == \"object\"].to_list(), drop_last=False)\n",
    "df_encoded = encoder.fit_transform(df_imputed)\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We know that this command returns a pandas Series, with the first value representing the correlation between 'Status' and itself, which is always 1. To exclude this, we slice the Series starting from index 1 using `[1:]`. Then, we sort the values by their absolute magnitude, using `key=abs` to ensure that the correlations are ordered by their strength, regardless of sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pearson = df_encoded.corr(method=\"pearson\")[\"Status\"].sort_values(key=abs, ascending=False)[1:].head(10)\n",
    "corr_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_spearman = df_encoded.corr(method=\"spearman\")[\"Status\"].sort_values(key=abs, ascending=False)[1:].head(10)\n",
    "corr_spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation studies, both Pearson and Spearman, did not reveal any strong relationships between the features and the target variable, with the exception of `credit_type_EQUI`. However, it's worth noting that credit_type_EQUI represents only about 10% of the total loan types, according to the profiling, which limits its significance. While this suggests that there may not be immediately obvious strong correlations, it does not imply that the model won't perform well. Instead, further data engineering and feature selection are needed. As a result, we will not focus further on the correlation analysis but will explore other techniques, such as selecting the best features, for the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "+ The correlation study did not provide significant insights into which features strongly influence the default status.\n",
    "+ The data will undergo further cleaning and imputation, followed by additional analysis to identify and select the most relevant features for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
