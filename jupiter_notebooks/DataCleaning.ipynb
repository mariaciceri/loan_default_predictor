{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "+ Imput missing data.\n",
    "+ Clean data.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "+ outs/datasets/collection/LoanDefault.csv\n",
    "\n",
    "## Outputs\n",
    "\n",
    "+ Generate a cleaned dataset and divide it into train and test sets.\n",
    "+ Save this in outputs/datasets/cleaned.\n",
    "\n",
    "---\n",
    "\n",
    "## Preparing for cleaning\n",
    "\n",
    "We must first change the working directory and then load the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd() # get the current working directory\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir)) # change directory to parent directory\n",
    "print(\"The directory you are in is:\", os.getcwd()) # print current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_limit                    3344\n",
       "approv_in_adv                  908\n",
       "loan_purpose                   134\n",
       "rate_of_interest             36439\n",
       "Interest_rate_spread         36639\n",
       "Upfront_charges              39642\n",
       "term                            41\n",
       "Neg_ammortization              121\n",
       "property_value               15098\n",
       "income                        9150\n",
       "age                            200\n",
       "submission_of_application      200\n",
       "LTV                          15098\n",
       "dtir1                        24121\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"outputs/datasets/collection/LoanDefault.csv\")\n",
    "df.isna().sum()[df.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confirming that there are many with missing values.\n",
    "we will impute like this this and that\n",
    "####################### fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loan_limit',\n",
       " 'approv_in_adv',\n",
       " 'loan_purpose',\n",
       " 'rate_of_interest',\n",
       " 'Interest_rate_spread',\n",
       " 'Upfront_charges',\n",
       " 'term',\n",
       " 'Neg_ammortization',\n",
       " 'property_value',\n",
       " 'income',\n",
       " 'age',\n",
       " 'submission_of_application',\n",
       " 'LTV',\n",
       " 'dtir1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_with_missing = df.columns[df.isna().any()].tolist()\n",
    "features_with_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data cleaning summary\n",
    "\n",
    "drop ID and year\n",
    "impute values to features with missing data (explain choices)\n",
    "\n",
    "capping LTV and property value\n",
    "\n",
    "################ fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import (CategoricalImputer,\n",
    "                                       MeanMedianImputer,\n",
    "                                       ArbitraryNumberImputer)\n",
    "from feature_engine.selection import DropFeatures\n",
    "from feature_engine.outliers import OutlierTrimmer\n",
    "\n",
    "\n",
    "# Drop ID and year columns\n",
    "drop_features = DropFeatures(features_to_drop=['ID', 'year'])\n",
    "df_cleaned = drop_features.fit_transform(df)\n",
    "\n",
    "# Impute missing values for categorical features\n",
    "impute_categorical = CategoricalImputer(imputation_method=\"frequent\")\n",
    "df_cleaned = impute_categorical.fit_transform(df_cleaned)\n",
    "\n",
    "# Impute missing values for numerical features\n",
    "median_variables = [\"Upfront_charges\",\n",
    "                    \"rate_of_interest\",\n",
    "                    \"property_value\",\n",
    "                    \"income\"]\n",
    "impute_median = MeanMedianImputer(imputation_method=\"median\",\n",
    "                                  variables=median_variables)\n",
    "df_cleaned = impute_median.fit_transform(df_cleaned)\n",
    "\n",
    "mean_variables = [\"Interest_rate_spread\", \"dtir1\"]\n",
    "impute_mean = MeanMedianImputer(imputation_method=\"mean\",\n",
    "                                variables=mean_variables)\n",
    "df_cleaned = impute_mean.fit_transform(df_cleaned)\n",
    "\n",
    "number_of_terms = df[\"term\"].max()\n",
    "impute_max = ArbitraryNumberImputer(arbitrary_number=number_of_terms,\n",
    "                                    variables=[\"term\"])\n",
    "df_cleaned = impute_max.fit_transform(df_cleaned)\n",
    "\n",
    "df_cleaned.loc[df_cleaned[\"LTV\"].isnull(), \"LTV\"] = (\n",
    "    df_cleaned[\"loan_amount\"] / df_cleaned[\"property_value\"] * 100\n",
    ")\n",
    "\n",
    "# Eliminate outliers from LTV as they don't make sense\n",
    "outliers = OutlierTrimmer(capping_method=\"quantiles\",\n",
    "                          fold=0.05,\n",
    "                          variables=[\"LTV\"])\n",
    "df_cleaned = outliers.fit_transform(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Check for missing values again\n",
    "df_cleaned.isna().sum()[df_cleaned.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainSet shape: (113075, 32) \n",
      "TestSet shape: (28269, 32)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "TrainSet, TestSet, _, __ = train_test_split(\n",
    "                                        df_cleaned,\n",
    "                                        df_cleaned['Status'],\n",
    "                                        test_size=0.2,\n",
    "                                        random_state=0)\n",
    "\n",
    "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try: \n",
    "    os.makedirs(\"outputs/datasets/cleaned\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet.to_csv(\"outputs/datasets/cleaned/TrainSet.csv\", index=False)\n",
    "TestSet.to_csv(\"outputs/datasets/cleaned/TestSet.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
